{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1: Linear Regression with One Feature\n",
    "## Stellar Luminosity Modeling\n",
    "\n",
    "**Course**: Enterprise Architecture (AREP) - ML Bootcamp\n",
    "**Objective**: Model stellar luminosity as a function of stellar mass using linear regression with an explicit bias term.\n",
    "\n",
    "**Model**: $\\hat{L} = w \\cdot M + b$\n",
    "\n",
    "Where:\n",
    "- $M$: stellar mass (in solar mass units, $M_\\odot$)\n",
    "- $L$: stellar luminosity (in solar luminosity units, $L_\\odot$)\n",
    "- $w$: slope (weight parameter)\n",
    "- $b$: intercept (bias parameter)\n"
   ],
   "id": "3eefddd49ad57d69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Dataset Visualization\n",
    "\n",
    "We begin by defining our dataset and visualizing the relationship between stellar mass and luminosity."
   ],
   "id": "351f26a97b6fe6b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ],
   "id": "a522d4ef613bec46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dataset: Stellar Mass (M_sun) vs Luminosity (L_sun)\n",
    "M = np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "L = np.array([0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0])\n",
    "\n",
    "m_samples = len(M)\n",
    "\n",
    "print(f\"Number of samples: {m_samples}\")\n",
    "print(f\"\\nStellar Mass (M_sun): {M}\")\n",
    "print(f\"Luminosity (L_sun): {L}\")"
   ],
   "id": "b7bf976f77508ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualization: Mass vs Luminosity\n",
    "plt.figure()\n",
    "plt.scatter(M, L, color='blue', s=80, alpha=0.7, edgecolors='black')\n",
    "plt.xlabel('Stellar Mass ($M_\\odot$)', fontsize=12)\n",
    "plt.ylabel('Stellar Luminosity ($L_\\odot$)', fontsize=12)\n",
    "plt.title('Stellar Mass vs Luminosity', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Analysis of Linearity ===\")\n",
    "print(\"Observation: The relationship between mass and luminosity appears NON-LINEAR.\")\n",
    "print(\"The luminosity increases rapidly (exponentially) with mass, not linearly.\")\n",
    "print(\"\\nAstrophysical Context:\")\n",
    "print(\"- Main sequence stars follow approximately L ∝ M^3.5 (mass-luminosity relation)\")\n",
    "print(\"- A linear model will provide a rough approximation but will have systematic errors\")\n",
    "print(\"- Higher mass stars will be underestimated, lower mass stars may be overestimated\")"
   ],
   "id": "3142623c34f69178"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Model and Loss Function\n",
    "\n",
    "### Linear Regression Model\n",
    "\n",
    "For a single feature, our hypothesis function is:\n",
    "\n",
    "$$\n",
    "\\hat{L}^{(i)} = f_{w,b}(M^{(i)}) = w \\cdot M^{(i)} + b\n",
    "$$\n",
    "\n",
    "### Mean Squared Error (MSE) Loss\n",
    "\n",
    "We measure the quality of our model using the MSE cost function:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{L}^{(i)} - L^{(i)} \\right)^2\n",
    "= \\frac{1}{2m} \\sum_{i=1}^{m} \\left( w \\cdot M^{(i)} + b - L^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of training examples\n",
    "- The factor $\\frac{1}{2m}$ is for mathematical convenience when computing derivatives"
   ],
   "id": "7a051281748a13d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict(M, w, b):\n",
    "    \"\"\"\n",
    "    Compute predictions for stellar luminosity.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : np.ndarray\n",
    "        Stellar mass values\n",
    "    w : float\n",
    "        Weight (slope)\n",
    "    b : float\n",
    "        Bias (intercept)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Predicted luminosity values\n",
    "    \"\"\"\n",
    "    return w * M + b\n",
    "\n",
    "\n",
    "def compute_cost(M, L, w, b):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error cost function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : np.ndarray\n",
    "        Stellar mass values\n",
    "    L : np.ndarray\n",
    "        Actual luminosity values\n",
    "    w : float\n",
    "        Weight (slope)\n",
    "    b : float\n",
    "        Bias (intercept)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Cost J(w, b)\n",
    "    \"\"\"\n",
    "    m = len(M)\n",
    "    predictions = predict(M, w, b)\n",
    "    errors = predictions - L\n",
    "    cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Test the functions with initial parameters\n",
    "w_init = 0.0\n",
    "b_init = 0.0\n",
    "\n",
    "L_pred_init = predict(M, w_init, b_init)\n",
    "cost_init = compute_cost(M, L, w_init, b_init)\n",
    "\n",
    "print(\"=== Initial Model Test (w=0, b=0) ===\")\n",
    "print(f\"Predictions: {L_pred_init}\")\n",
    "print(f\"Initial Cost J(0, 0) = {cost_init:.4f}\")"
   ],
   "id": "2c6d1f802a16f82a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Cost Surface Visualization (3D and Contour)\n",
    "\n",
    "To understand the optimization landscape, we visualize the cost function $J(w, b)$ over a grid of parameter values.\n",
    "\n",
    "The minimum of this surface represents the optimal parameters that best fit our data."
   ],
   "id": "a51bc61707083967"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a grid of w and b values\n",
    "w_range = np.linspace(0, 30, 100)\n",
    "b_range = np.linspace(-10, 10, 100)\n",
    "W_grid, B_grid = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# Compute cost for each (w, b) pair\n",
    "J_grid = np.zeros_like(W_grid)\n",
    "\n",
    "for i in range(W_grid.shape[0]):\n",
    "    for j in range(W_grid.shape[1]):\n",
    "        J_grid[i, j] = compute_cost(M, L, W_grid[i, j], B_grid[i, j])\n",
    "\n",
    "print(\"Cost surface computed successfully.\")\n",
    "print(f\"Minimum cost in grid: {np.min(J_grid):.4f}\")\n",
    "print(f\"Maximum cost in grid: {np.max(J_grid):.4f}\")"
   ],
   "id": "f5e61a1073e13e48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3D Surface Plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Left subplot: 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(W_grid, B_grid, J_grid, cmap=cm.viridis,\n",
    "                        alpha=0.8, linewidth=0, antialiased=True)\n",
    "ax1.set_xlabel('Weight (w)', fontsize=10)\n",
    "ax1.set_ylabel('Bias (b)', fontsize=10)\n",
    "ax1.set_zlabel('Cost J(w, b)', fontsize=10)\n",
    "ax1.set_title('Cost Surface (3D View)', fontsize=12, fontweight='bold')\n",
    "ax1.view_init(elev=25, azim=45)\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n",
    "\n",
    "# Right subplot: Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W_grid, B_grid, J_grid, levels=20, cmap=cm.viridis)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('Weight (w)', fontsize=10)\n",
    "ax2.set_ylabel('Bias (b)', fontsize=10)\n",
    "ax2.set_title('Cost Surface (Contour View)', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Cost Surface Interpretation ===\")\n",
    "print(\"The cost surface shows a convex bowl shape, indicating:\")\n",
    "print(\"1. A single global minimum exists (no local minima)\")\n",
    "print(\"2. The minimum represents the optimal (w, b) that best fits the data\")\n",
    "print(\"3. Gradient descent will converge to this minimum from any starting point\")\n",
    "print(\"4. The contour lines show level sets of equal cost\")"
   ],
   "id": "f411c4f50984f54b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Gradient Computation\n",
    "\n",
    "To minimize the cost function, we need the partial derivatives (gradients) with respect to $w$ and $b$.\n",
    "\n",
    "### Derivation:\n",
    "\n",
    "Starting from:\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( w \\cdot M^{(i)} + b - L^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "**Gradient with respect to w:**\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( w \\cdot M^{(i)} + b - L^{(i)} \\right) \\cdot M^{(i)}\n",
    "$$\n",
    "\n",
    "**Gradient with respect to b:**\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( w \\cdot M^{(i)} + b - L^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "These gradients tell us the direction of steepest increase in cost. We move in the opposite direction to minimize cost."
   ],
   "id": "8b5327db446d18b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Non-Vectorized Gradient Descent\n",
    "\n",
    "We first implement gradient descent using explicit loops over all samples to compute gradients.\n",
    "\n",
    "This approach is more intuitive but less efficient than vectorized operations."
   ],
   "id": "b854fa200de197d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_gradients_loop(M, L, w, b):\n",
    "    \"\"\"\n",
    "    Compute gradients using explicit loops (non-vectorized).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : np.ndarray\n",
    "        Stellar mass values\n",
    "    L : np.ndarray\n",
    "        Actual luminosity values\n",
    "    w : float\n",
    "        Current weight\n",
    "    b : float\n",
    "        Current bias\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (float, float)\n",
    "        Gradients (dJ_dw, dJ_db)\n",
    "    \"\"\"\n",
    "    m = len(M)\n",
    "    dJ_dw = 0.0\n",
    "    dJ_db = 0.0\n",
    "\n",
    "    # Loop over all training examples\n",
    "    for i in range(m):\n",
    "        prediction = w * M[i] + b\n",
    "        error = prediction - L[i]\n",
    "\n",
    "        dJ_dw += error * M[i]\n",
    "        dJ_db += error\n",
    "\n",
    "    # Average over all samples\n",
    "    dJ_dw /= m\n",
    "    dJ_db /= m\n",
    "\n",
    "    return dJ_dw, dJ_db\n",
    "\n",
    "\n",
    "def gradient_descent_loop(M, L, w_init, b_init, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent using loop-based gradient computation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : np.ndarray\n",
    "        Stellar mass values\n",
    "    L : np.ndarray\n",
    "        Actual luminosity values\n",
    "    w_init : float\n",
    "        Initial weight\n",
    "    b_init : float\n",
    "        Initial bias\n",
    "    learning_rate : float\n",
    "        Step size (alpha)\n",
    "    num_iterations : int\n",
    "        Number of gradient descent iterations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (w_final, b_final, cost_history)\n",
    "    \"\"\"\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Compute gradients\n",
    "        dJ_dw, dJ_db = compute_gradients_loop(M, L, w, b)\n",
    "\n",
    "        # Update parameters\n",
    "        w = w - learning_rate * dJ_dw\n",
    "        b = b - learning_rate * dJ_db\n",
    "\n",
    "        # Record cost\n",
    "        cost = compute_cost(M, L, w, b)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print progress\n",
    "        if i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(f\"Iteration {i:4d}: Cost = {cost:8.4f}, w = {w:7.4f}, b = {b:7.4f}\")\n",
    "\n",
    "    return w, b, cost_history\n",
    "\n",
    "\n",
    "# Test non-vectorized gradient computation\n",
    "print(\"=== Testing Non-Vectorized Gradient Computation ===\")\n",
    "dJ_dw_test, dJ_db_test = compute_gradients_loop(M, L, 0.0, 0.0)\n",
    "print(f\"Gradients at (w=0, b=0):\")\n",
    "print(f\"  dJ/dw = {dJ_dw_test:.4f}\")\n",
    "print(f\"  dJ/db = {dJ_db_test:.4f}\")"
   ],
   "id": "86d91e2956448266"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Vectorized Gradient Descent\n",
    "\n",
    "Now we implement the same algorithm using NumPy's vectorized operations.\n",
    "\n",
    "This approach is much faster and more efficient, especially for large datasets."
   ],
   "id": "8cefa2932818ef0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_gradients_vectorized(M, L, w, b):\n",
    "    \"\"\"\n",
    "    Compute gradients using vectorized operations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : np.ndarray\n",
    "        Stellar mass values\n",
    "    L : np.ndarray\n",
    "        Actual luminosity values\n",
    "    w : float\n",
    "        Current weight\n",
    "    b : float\n",
    "        Current bias\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (float, float)\n",
    "        Gradients (dJ_dw, dJ_db)\n",
    "    \"\"\"\n",
    "    m = len(M)\n",
    "\n",
    "    # Vectorized computation\n",
    "    predictions = w * M + b\n",
    "    errors = predictions - L\n",
    "\n",
    "    dJ_dw = (1 / m) * np.sum(errors * M)\n",
    "    dJ_db = (1 / m) * np.sum(errors)\n",
    "\n",
    "    return dJ_dw, dJ_db\n",
    "\n",
    "\n",
    "def gradient_descent_vectorized(M, L, w_init, b_init, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent using vectorized gradient computation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : np.ndarray\n",
    "        Stellar mass values\n",
    "    L : np.ndarray\n",
    "        Actual luminosity values\n",
    "    w_init : float\n",
    "        Initial weight\n",
    "    b_init : float\n",
    "        Initial bias\n",
    "    learning_rate : float\n",
    "        Step size (alpha)\n",
    "    num_iterations : int\n",
    "        Number of gradient descent iterations\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (w_final, b_final, cost_history)\n",
    "    \"\"\"\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Compute gradients (vectorized)\n",
    "        dJ_dw, dJ_db = compute_gradients_vectorized(M, L, w, b)\n",
    "\n",
    "        # Update parameters\n",
    "        w = w - learning_rate * dJ_dw\n",
    "        b = b - learning_rate * dJ_db\n",
    "\n",
    "        # Record cost\n",
    "        cost = compute_cost(M, L, w, b)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print progress\n",
    "        if i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(f\"Iteration {i:4d}: Cost = {cost:8.4f}, w = {w:7.4f}, b = {b:7.4f}\")\n",
    "\n",
    "    return w, b, cost_history\n",
    "\n",
    "\n",
    "# Test vectorized gradient computation\n",
    "print(\"=== Testing Vectorized Gradient Computation ===\")\n",
    "dJ_dw_vec, dJ_db_vec = compute_gradients_vectorized(M, L, 0.0, 0.0)\n",
    "print(f\"Gradients at (w=0, b=0):\")\n",
    "print(f\"  dJ/dw = {dJ_dw_vec:.4f}\")\n",
    "print(f\"  dJ/db = {dJ_db_vec:.4f}\")\n",
    "\n",
    "# Verify both methods produce the same results\n",
    "print(f\"\\nGradient computation methods match: {np.allclose(dJ_dw_test, dJ_dw_vec) and np.allclose(dJ_db_test, dJ_db_vec)}\")"
   ],
   "id": "8d02dcb668dbf246"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Convergence Analysis\n",
    "\n",
    "We train the model and analyze how the cost decreases over iterations.\n",
    "\n",
    "A properly configured gradient descent should show:\n",
    "- Monotonic decrease in cost\n",
    "- Eventual convergence to a stable minimum\n",
    "- Smooth convergence curve (no oscillations)"
   ],
   "id": "37bca6da1c420d18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train model with vectorized gradient descent\n",
    "print(\"=== Training Model (Vectorized Implementation) ===\")\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "w_final, b_final, cost_history = gradient_descent_vectorized(\n",
    "    M, L, w_init=0.0, b_init=0.0,\n",
    "    learning_rate=learning_rate,\n",
    "    num_iterations=num_iterations\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Final Results ===\")\n",
    "print(f\"Learned weight (w): {w_final:.4f}\")\n",
    "print(f\"Learned bias (b): {b_final:.4f}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")"
   ],
   "id": "7c9d120f406c640d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot convergence curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history, linewidth=2, color='blue')\n",
    "plt.xlabel('Iteration', fontsize=11)\n",
    "plt.ylabel('Cost J(w, b)', fontsize=11)\n",
    "plt.title('Cost vs Iterations (Full Range)', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cost_history[50:], linewidth=2, color='green')\n",
    "plt.xlabel('Iteration', fontsize=11)\n",
    "plt.ylabel('Cost J(w, b)', fontsize=11)\n",
    "plt.title('Cost vs Iterations (After Iteration 50)', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Convergence Analysis ===\")\n",
    "print(f\"Initial cost: {cost_history[0]:.4f}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")\n",
    "print(f\"Cost reduction: {cost_history[0] - cost_history[-1]:.4f} ({100 * (cost_history[0] - cost_history[-1]) / cost_history[0]:.2f}%)\")\n",
    "print(f\"\\nConvergence speed: Fast initial decrease, then gradual refinement\")\n",
    "print(f\"Stability: Smooth monotonic decrease with no oscillations\")\n",
    "print(f\"Status: Successfully converged to minimum\")"
   ],
   "id": "3d85bd608e9dba30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8. Learning Rate Experiments\n",
    "\n",
    "The learning rate $\\alpha$ is a critical hyperparameter that controls the step size in gradient descent.\n",
    "\n",
    "We experiment with three different learning rates to observe their effects:\n",
    "- **Too small**: Slow convergence\n",
    "- **Optimal**: Fast and stable convergence\n",
    "- **Too large**: Potential oscillation or divergence"
   ],
   "id": "8837aa51c5084957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment with different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.05]\n",
    "results = {}\n",
    "\n",
    "print(\"=== Learning Rate Experiments ===\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nLearning Rate: {lr}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    w, b, history = gradient_descent_vectorized(\n",
    "        M, L, w_init=0.0, b_init=0.0,\n",
    "        learning_rate=lr,\n",
    "        num_iterations=1000\n",
    "    )\n",
    "\n",
    "    results[lr] = {\n",
    "        'w': w,\n",
    "        'b': b,\n",
    "        'final_cost': history[-1],\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "    print(f\"\\nFinal Parameters:\")\n",
    "    print(f\"  w = {w:.4f}\")\n",
    "    print(f\"  b = {b:.4f}\")\n",
    "    print(f\"  Final Cost = {history[-1]:.4f}\")"
   ],
   "id": "81dc84bed42a53de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compare convergence for different learning rates\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Full range\n",
    "plt.subplot(1, 2, 1)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(results[lr]['history'], label=f'α = {lr}', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=11)\n",
    "plt.ylabel('Cost J(w, b)', fontsize=11)\n",
    "plt.title('Convergence Comparison (Full Range)', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# After iteration 50\n",
    "plt.subplot(1, 2, 2)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(range(50, len(results[lr]['history'])),\n",
    "             results[lr]['history'][50:], label=f'α = {lr}', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=11)\n",
    "plt.ylabel('Cost J(w, b)', fontsize=11)\n",
    "plt.title('Convergence Comparison (After Iteration 50)', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Learning Rate Comparison Summary ===\")\n",
    "print(f\"\\n{'Learning Rate':<15} {'Final w':<12} {'Final b':<12} {'Final Cost':<12} {'Convergence'}\")\n",
    "print(\"-\" * 80)\n",
    "for lr in learning_rates:\n",
    "    r = results[lr]\n",
    "    speed = \"Slow\" if lr == 0.001 else (\"Optimal\" if lr == 0.01 else \"Fast\")\n",
    "    print(f\"{lr:<15.3f} {r['w']:<12.4f} {r['b']:<12.4f} {r['final_cost']:<12.4f} {speed}\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- α = 0.001: Slowest convergence, needs more iterations\")\n",
    "print(\"- α = 0.01:  Good balance between speed and stability (recommended)\")\n",
    "print(\"- α = 0.05:  Fastest convergence, but may overshoot in other problems\")"
   ],
   "id": "b2e26389e6794bcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9. Final Model Fit Visualization\n",
    "\n",
    "We plot the learned regression line over the training data to visually assess the model's performance.\n",
    "\n",
    "This allows us to identify systematic errors and understand the model's limitations."
   ],
   "id": "6a5de44b55ba7404"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the best model (learning rate = 0.01)\n",
    "w_best = results[0.01]['w']\n",
    "b_best = results[0.01]['b']\n",
    "\n",
    "# Generate predictions\n",
    "L_pred = predict(M, w_best, b_best)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(M, L, color='blue', s=100, alpha=0.7, edgecolors='black',\n",
    "            label='Actual Data', zorder=3)\n",
    "plt.plot(M, L_pred, color='red', linewidth=2.5,\n",
    "         label=f'Linear Fit: L = {w_best:.2f}M + {b_best:.2f}', zorder=2)\n",
    "\n",
    "# Add vertical lines showing errors\n",
    "for i in range(len(M)):\n",
    "    plt.plot([M[i], M[i]], [L[i], L_pred[i]], 'k--', alpha=0.3, linewidth=1)\n",
    "\n",
    "plt.xlabel('Stellar Mass ($M_\\odot$)', fontsize=12)\n",
    "plt.ylabel('Stellar Luminosity ($L_\\odot$)', fontsize=12)\n",
    "plt.title('Linear Regression Model: Stellar Mass vs Luminosity',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute residuals (errors)\n",
    "residuals = L - L_pred\n",
    "\n",
    "print(\"\\n=== Model Performance Analysis ===\")\n",
    "print(f\"\\nLearned Model: L = {w_best:.4f} * M + {b_best:.4f}\")\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {np.mean(np.abs(residuals)):.4f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): {np.sqrt(np.mean(residuals**2)):.4f}\")\n",
    "print(f\"  Max positive residual: {np.max(residuals):.4f} (underestimation)\")\n",
    "print(f\"  Max negative residual: {np.min(residuals):.4f} (overestimation)\")\n",
    "\n",
    "print(\"\\n=== Systematic Error Analysis ===\")\n",
    "print(\"Observations:\")\n",
    "print(\"1. Low-mass stars (M < 1.0): Model tends to overestimate luminosity\")\n",
    "print(\"2. High-mass stars (M > 2.0): Model significantly underestimates luminosity\")\n",
    "print(\"3. The errors are NOT random - they show a clear pattern\")\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"The linear model is fundamentally limited because the true relationship\")\n",
    "print(\"is non-linear (approximately L ∝ M^3.5 for main sequence stars).\")\n",
    "print(\"A polynomial model would provide a much better fit.\")"
   ],
   "id": "c6b568c90b66016b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Residual plot to visualize systematic errors\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(M, residuals, color='purple', s=80, alpha=0.7, edgecolors='black')\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "plt.xlabel('Stellar Mass ($M_\\odot$)', fontsize=11)\n",
    "plt.ylabel('Residual (Actual - Predicted)', fontsize=11)\n",
    "plt.title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(L_pred, residuals, color='orange', s=80, alpha=0.7, edgecolors='black')\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "plt.xlabel('Predicted Luminosity ($L_\\odot$)', fontsize=11)\n",
    "plt.ylabel('Residual (Actual - Predicted)', fontsize=11)\n",
    "plt.title('Residual vs Predicted', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Residual Plot Interpretation ===\")\n",
    "print(\"The residual plots clearly show:\")\n",
    "print(\"1. Non-random pattern: Residuals follow a curve, not scattered around zero\")\n",
    "print(\"2. Systematic bias: Positive residuals at extremes, negative in middle\")\n",
    "print(\"3. Heteroscedasticity: Error magnitude increases with mass\")\n",
    "print(\"\\nThis confirms the need for a non-linear (polynomial) model.\")"
   ],
   "id": "b3905c3e708ffc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Conceptual Questions and Astrophysical Interpretation\n",
    "\n",
    "### Question 1: What is the astrophysical meaning of the weight parameter $w$?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The weight $w$ represents the **rate of change of luminosity with respect to mass** in our linear approximation.\n",
    "\n",
    "In our model:\n",
    "$$\n",
    "\\hat{L} = w \\cdot M + b\n",
    "$$\n",
    "\n",
    "The parameter $w$ tells us: *\"For each additional solar mass, the luminosity increases by approximately $w$ solar luminosities.\"*\n",
    "\n",
    "From our trained model:\n",
    "- $w \\approx 19.3$\n",
    "\n",
    "This means: **Adding 1 $M_\\odot$ increases luminosity by approximately 19.3 $L_\\odot$** (in our linear approximation).\n",
    "\n",
    "**Astrophysical Context:**\n",
    "\n",
    "In reality, the mass-luminosity relation for main sequence stars follows:\n",
    "$$\n",
    "L \\propto M^{\\alpha}, \\quad \\text{where } \\alpha \\approx 3.5\n",
    "$$\n",
    "\n",
    "This power-law relationship means:\n",
    "- Low-mass stars are much dimmer than our linear model predicts\n",
    "- High-mass stars are much brighter than our linear model predicts\n",
    "- The \"slope\" in a linear model is an average approximation that doesn't capture the true physics\n",
    "\n",
    "The bias term $b$ has less physical meaning—it's mainly a mathematical adjustment to minimize error across our data range.\n",
    "\n",
    "### Question 2: Why is a linear model fundamentally limited here?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "A linear model is limited because **stellar luminosity depends non-linearly on mass**.\n",
    "\n",
    "**Physical Reasons:**\n",
    "\n",
    "1. **Nuclear fusion rate**: The rate of hydrogen fusion in a star's core depends on temperature and pressure, which scale non-linearly with mass\n",
    "\n",
    "2. **Energy transport**: More massive stars have higher core temperatures, leading to exponentially higher energy generation rates\n",
    "\n",
    "3. **Radiative efficiency**: The efficiency of energy transport from core to surface changes with mass\n",
    "\n",
    "4. **Empirical evidence**: Observations show $L \\propto M^{3.5}$ for main sequence stars\n",
    "\n",
    "**Mathematical Evidence from our Analysis:**\n",
    "\n",
    "Our residual analysis clearly shows:\n",
    "- Systematic underestimation for high-mass stars\n",
    "- Systematic overestimation for low-mass stars\n",
    "- Non-random error pattern (curved residual plot)\n",
    "\n",
    "**Improvement Strategy:**\n",
    "\n",
    "To better model this relationship, we should:\n",
    "1. Use polynomial regression (e.g., $L = w_1 M + w_2 M^2 + w_3 M^3 + b$)\n",
    "2. Or use log-transformed variables: $\\log L = \\alpha \\log M + c$\n",
    "3. Include additional features like temperature (which we'll do in Part 2)\n",
    "\n",
    "These approaches can capture the non-linear mass-luminosity relationship more accurately."
   ],
   "id": "d7c4d1240e035d38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Demonstrate the limitation with a comparison\n",
    "print(\"=== Quantitative Demonstration of Model Limitations ===\")\n",
    "print(f\"\\nFor a 0.6 M_sun star:\")\n",
    "print(f\"  Actual luminosity: {L[0]:.2f} L_sun\")\n",
    "print(f\"  Predicted luminosity: {L_pred[0]:.2f} L_sun\")\n",
    "print(f\"  Error: {residuals[0]:.2f} L_sun ({100*residuals[0]/L[0]:.1f}%)\")\n",
    "\n",
    "print(f\"\\nFor a 2.4 M_sun star:\")\n",
    "print(f\"  Actual luminosity: {L[-1]:.2f} L_sun\")\n",
    "print(f\"  Predicted luminosity: {L_pred[-1]:.2f} L_sun\")\n",
    "print(f\"  Error: {residuals[-1]:.2f} L_sun ({100*residuals[-1]/L[-1]:.1f}%)\")\n",
    "\n",
    "print(f\"\\nThe linear model's relative error increases dramatically for high-mass stars!\")\n",
    "print(f\"This is because luminosity grows much faster than linearly with mass.\")"
   ],
   "id": "1758eb9efe13e9ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. **Implemented linear regression from scratch** using only NumPy\n",
    "2. **Derived and coded** the cost function and its gradients\n",
    "3. **Implemented gradient descent** in both non-vectorized and vectorized forms\n",
    "4. **Visualized** the optimization landscape (cost surface)\n",
    "5. **Experimented** with different learning rates\n",
    "6. **Analyzed** convergence behavior and model performance\n",
    "7. **Identified** systematic errors and model limitations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Learned Model:**\n",
    "$$\n",
    "L \\approx 19.3 \\cdot M - 6.8\n",
    "$$\n",
    "\n",
    "**Performance:**\n",
    "- Training cost converged successfully\n",
    "- However, significant systematic errors remain\n",
    "\n",
    "**Limitations:**\n",
    "- Linear model cannot capture the true power-law relationship ($L \\propto M^{3.5}$)\n",
    "- Underestimates high-mass star luminosities\n",
    "- Overestimates low-mass star luminosities\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In **Part 2**, we will:\n",
    "- Add temperature as a second feature\n",
    "- Use polynomial features ($M^2$, $M \\times T$)\n",
    "- Compare different feature combinations\n",
    "- Achieve better model accuracy through feature engineering\n",
    "\n",
    "This demonstrates a fundamental principle of machine learning: **model capacity must match problem complexity**."
   ],
   "id": "9c8c9bc9647c80d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Final summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1 COMPLETE: LINEAR REGRESSION WITH ONE FEATURE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nFinal Model Parameters:\")\n",
    "print(f\"  Weight (w): {w_best:.4f}\")\n",
    "print(f\"  Bias (b): {b_best:.4f}\")\n",
    "print(f\"\\nFinal Cost: {results[0.01]['final_cost']:.4f}\")\n",
    "print(f\"\\nModel Equation: L = {w_best:.2f} * M + ({b_best:.2f})\")\n",
    "print(f\"\\nRecommended Learning Rate: 0.01\")\n",
    "print(f\"Iterations Required: 1000\")\n",
    "print(f\"\\nConclusion: Linear model provides baseline but needs polynomial features for accuracy.\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "d4566bb3033742d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2d2307ccfd7f33f4"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
